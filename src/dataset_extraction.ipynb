{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "df8bcf75",
   "metadata": {},
   "outputs": [],
   "source": [
    "import zipfile\n",
    "import os\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fdf2fef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- CONFIG ----\n",
    "# Path to the outer .zip file\n",
    "data_zip_path = \"multi_step_attacks_dataset.zip\"\n",
    "\n",
    "# Extraction folder\n",
    "extract_dir = \"extracted_dataset\"\n",
    "inner_extract_dir = os.path.join(extract_dir, \"inner\")\n",
    "parquet_filename = \"flattened_dataset.parquet\"\n",
    "\n",
    "# ---------------- Extract outer ZIP ----------------\n",
    "with zipfile.ZipFile(data_zip_path, 'r') as outer_zip:\n",
    "    outer_zip.extractall(extract_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7f8f9f5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted extracted_dataset\\inner\\fox_aminer.json\n",
      "Deleted extracted_dataset\\inner\\harrison_aminer.json\n",
      "Deleted extracted_dataset\\inner\\russellmitchell_aminer.json\n",
      "Deleted extracted_dataset\\inner\\santos_aminer.json\n",
      "Deleted extracted_dataset\\inner\\shaw_aminer.json\n",
      "Deleted extracted_dataset\\inner\\wardbeck_aminer.json\n",
      "Deleted extracted_dataset\\inner\\wheeler_aminer.json\n",
      "Deleted extracted_dataset\\inner\\wilson_aminer.json\n"
     ]
    }
   ],
   "source": [
    "# Find inner zip file\n",
    "inner_zip_path = None\n",
    "for root, dirs, files in os.walk(extract_dir):\n",
    "    for f in files:\n",
    "        if f.endswith(\".zip\"):\n",
    "            inner_zip_path = os.path.join(root, f)\n",
    "            break\n",
    "\n",
    "\n",
    "if inner_zip_path:\n",
    "    os.makedirs(inner_extract_dir, exist_ok=True)\n",
    "    with zipfile.ZipFile(inner_zip_path, 'r') as inner_zip:\n",
    "        inner_zip.extractall(inner_extract_dir)\n",
    "\n",
    "    # Delete all JSON files that end with 'aminer.json'\n",
    "    for root, dirs, files in os.walk(inner_extract_dir):\n",
    "        for f in files:\n",
    "            if f.endswith(\"aminer.json\"):\n",
    "                file_path = os.path.join(root, f)\n",
    "                try:\n",
    "                    os.remove(file_path)\n",
    "                    print(f\"Deleted {file_path}\")\n",
    "                except Exception as e:\n",
    "                    print(f\"Could not delete {file_path}: {e}\")\n",
    "else:\n",
    "    raise FileNotFoundError(\"No inner zip file found inside dataset.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cf940e52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Labels Dataset Info ---\n",
      "          scenario         attack         start           end\n",
      "0  russellmitchell  network_scans  1.642993e+09  1.642997e+09\n",
      "1  russellmitchell  service_scans  1.642997e+09  1.642997e+09\n",
      "2  russellmitchell           dirb  1.642997e+09  1.642997e+09\n",
      "3  russellmitchell         wpscan  1.642997e+09  1.642997e+09\n",
      "4  russellmitchell       webshell  1.642997e+09  1.642997e+09\n",
      "\n",
      "Shape: (79, 4)\n",
      "\n",
      "Fields: ['scenario', 'attack', 'start', 'end']\n"
     ]
    }
   ],
   "source": [
    "# ---------------- Load labels.csv ----------------\n",
    "labels_path = os.path.join(extract_dir, \"labels.csv\")\n",
    "if os.path.exists(labels_path):\n",
    "    labels_df = pd.read_csv(labels_path, sep=\",\")\n",
    "    print(\"\\n--- Labels Dataset Info ---\")\n",
    "    print(labels_df.head())\n",
    "    print(\"\\nShape:\", labels_df.shape)\n",
    "    print(\"\\nFields:\", list(labels_df.columns))\n",
    "else:\n",
    "    print(\"labels.csv not found inside inner zip\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35289020",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded fox_wazuh.json with 462523 rows\n",
      "Loaded harrison_wazuh.json with 583754 rows\n",
      "Loaded russellmitchell_wazuh.json with 41488 rows\n",
      "Loaded santos_wazuh.json with 126513 rows\n",
      "Loaded shaw_wazuh.json with 68539 rows\n",
      "Loaded wardbeck_wazuh.json with 88204 rows\n",
      "Loaded wheeler_wazuh.json with 603939 rows\n",
      "Loaded wilson_wazuh.json with 625303 rows\n",
      "\n",
      "--- Combined Dataset ---\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 2600263 entries, 0 to 2600262\n",
      "Data columns (total 14 columns):\n",
      " #   Column           Dtype  \n",
      "---  ------           -----  \n",
      " 0   predecoder       object \n",
      " 1   agent            object \n",
      " 2   manager          object \n",
      " 3   rule             object \n",
      " 4   decoder          object \n",
      " 5   full_log         object \n",
      " 6   input            object \n",
      " 7   @timestamp       object \n",
      " 8   location         object \n",
      " 9   id               float64\n",
      " 10  data             object \n",
      " 11  GeoLocation      object \n",
      " 12  previous_output  object \n",
      " 13  filename         object \n",
      "dtypes: float64(1), object(13)\n",
      "memory usage: 277.7+ MB\n",
      "None\n",
      "Index(['predecoder', 'agent', 'manager', 'rule', 'decoder', 'full_log',\n",
      "       'input', '@timestamp', 'location', 'id', 'data', 'GeoLocation',\n",
      "       'previous_output', 'filename'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# ---------------- Load JSON files ----------------\n",
    "\n",
    "all_dfs = []\n",
    "\n",
    "for root, dirs, files in os.walk(inner_extract_dir):\n",
    "    for f in files:\n",
    "        if f.endswith(\".json\"):\n",
    "            json_path = os.path.join(root, f)\n",
    "            try:\n",
    "                df = pd.read_json(json_path, lines=True)  # NDJSON support\n",
    "                df['filename'] = f.split(\"_\")[0]  # Add source filename column\n",
    "                all_dfs.append(df)\n",
    "                print(f\"Loaded {f} with {len(df)} rows\")\n",
    "            except Exception as e:\n",
    "                print(f\"Could not read {f}: {e}\")\n",
    "\n",
    "# Combine into one big DataFrame\n",
    "data = pd.concat(all_dfs, ignore_index=True)\n",
    "print(\"\\n--- Combined Dataset ---\")\n",
    "print(data.info())\n",
    "print(data.columns)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "94f8b7b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns in predecoder: ['hostname', 'program_name', 'timestamp']\n",
      "Columns in agent: ['ip', 'name', 'id']\n",
      "Columns in manager: ['name']\n",
      "Columns in data: ['srcip', 'dstip', 'id', 'tx_id', 'app_proto', 'in_iface', 'src_ip', 'src_port', 'event_type', 'flow_id', 'dest_ip', 'proto', 'dest_port', 'timestamp', 'alert_severity', 'alert_signature_id', 'alert_rev', 'alert_metadata_updated_at', 'alert_metadata_former_category', 'alert_metadata_created_at', 'alert_gid', 'alert_signature', 'alert_action', 'alert_category', 'http_hostname', 'http_protocol', 'http_http_method', 'http_length', 'http_url', 'http_http_user_agent', 'http_status', 'flow_pkts_toserver', 'flow_start', 'flow_bytes_toclient', 'flow_bytes_toserver', 'flow_pkts_toclient', 'audit_uid', 'audit_auid', 'audit_gid', 'audit_exe', 'audit_euid', 'audit_session', 'audit_pid', 'audit_id', 'audit_type', 'audit_command', 'metadata_flowints_tls_anomaly_count', 'tls_version', 'tls_ja3_string', 'tls_ja3_hash', 'tls_sni', 'dstuser', 'srcport', 'uid', 'srcuser', 'tty', 'pwd', 'command', 'protocol', 'url', 'euid', 'dns_query', 'alert_metadata_affected_product', 'alert_metadata_attack_target', 'alert_metadata_signature_severity', 'alert_metadata_deployment', 'metadata_flowints_http_anomaly_count', 'http_http_port', 'files', 'http_http_content_type', 'tls_ja3s_string', 'tls_ja3s_hash', 'metadata_flowints_smtp_anomaly_count', 'app_proto_tc', 'alert_metadata_performance_impact', 'tls_serial', 'tls_notbefore', 'tls_subject', 'tls_issuerdn', 'tls_notafter', 'tls_fingerprint', 'http_redirect', 'http_http_refer', 'tls_session_resumed']\n",
      "Columns in rule: ['firedtimes', 'mail', 'level', 'pci_dss', 'tsc', 'description', 'groups', 'id', 'nist_800_53', 'gpg13', 'gdpr', 'hipaa', 'frequency', 'mitre_technique', 'mitre_id', 'mitre_tactic']\n",
      "Columns in decoder: ['name', 'parent', 'ftscomment']\n",
      "Columns in input: ['type']\n",
      "Columns in GeoLocation: ['city_name', 'country_name', 'region_name', 'location_lon', 'location_lat']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>full_log</th>\n",
       "      <th>@timestamp</th>\n",
       "      <th>location</th>\n",
       "      <th>id</th>\n",
       "      <th>previous_output</th>\n",
       "      <th>filename</th>\n",
       "      <th>predecoder_hostname</th>\n",
       "      <th>predecoder_program_name</th>\n",
       "      <th>predecoder_timestamp</th>\n",
       "      <th>agent_ip</th>\n",
       "      <th>...</th>\n",
       "      <th>rule_mitre_tactic</th>\n",
       "      <th>decoder_name</th>\n",
       "      <th>decoder_parent</th>\n",
       "      <th>decoder_ftscomment</th>\n",
       "      <th>input_type</th>\n",
       "      <th>GeoLocation_city_name</th>\n",
       "      <th>GeoLocation_country_name</th>\n",
       "      <th>GeoLocation_region_name</th>\n",
       "      <th>GeoLocation_location_lon</th>\n",
       "      <th>GeoLocation_location_lat</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Jan 15 02:32:32 mail freshclam[29266]: Sat Jan...</td>\n",
       "      <td>2022-01-15T02:32:32.000000Z</td>\n",
       "      <td>/var/log/syslog</td>\n",
       "      <td>1.686147e+09</td>\n",
       "      <td>NaN</td>\n",
       "      <td>fox</td>\n",
       "      <td>mail</td>\n",
       "      <td>freshclam</td>\n",
       "      <td>Jan 15 02:32:32</td>\n",
       "      <td>172.17.131.81</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>freshclam</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>log</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Jan 15 02:32:32 taylorcruz-mail freshclam[2851...</td>\n",
       "      <td>2022-01-15T02:32:32.000000Z</td>\n",
       "      <td>/var/log/syslog</td>\n",
       "      <td>1.686147e+09</td>\n",
       "      <td>NaN</td>\n",
       "      <td>fox</td>\n",
       "      <td>taylorcruz-mail</td>\n",
       "      <td>freshclam</td>\n",
       "      <td>Jan 15 02:32:32</td>\n",
       "      <td>192.168.128.170</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>freshclam</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>log</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Jan 15 02:32:37 mail freshclam[29266]: Sat Jan...</td>\n",
       "      <td>2022-01-15T02:32:37.000000Z</td>\n",
       "      <td>/var/log/syslog</td>\n",
       "      <td>1.686147e+09</td>\n",
       "      <td>NaN</td>\n",
       "      <td>fox</td>\n",
       "      <td>mail</td>\n",
       "      <td>freshclam</td>\n",
       "      <td>Jan 15 02:32:37</td>\n",
       "      <td>172.17.131.81</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>freshclam</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>log</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Jan 15 02:32:42 mail freshclam[29266]: Sat Jan...</td>\n",
       "      <td>2022-01-15T02:32:42.000000Z</td>\n",
       "      <td>/var/log/syslog</td>\n",
       "      <td>1.686147e+09</td>\n",
       "      <td>NaN</td>\n",
       "      <td>fox</td>\n",
       "      <td>mail</td>\n",
       "      <td>freshclam</td>\n",
       "      <td>Jan 15 02:32:42</td>\n",
       "      <td>172.17.131.81</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>freshclam</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>log</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Jan 15 02:32:47 mail freshclam[29266]: Sat Jan...</td>\n",
       "      <td>2022-01-15T02:32:47.000000Z</td>\n",
       "      <td>/var/log/syslog</td>\n",
       "      <td>1.686147e+09</td>\n",
       "      <td>NaN</td>\n",
       "      <td>fox</td>\n",
       "      <td>mail</td>\n",
       "      <td>freshclam</td>\n",
       "      <td>Jan 15 02:32:47</td>\n",
       "      <td>172.17.131.81</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>freshclam</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>log</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 122 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            full_log  \\\n",
       "0  Jan 15 02:32:32 mail freshclam[29266]: Sat Jan...   \n",
       "1  Jan 15 02:32:32 taylorcruz-mail freshclam[2851...   \n",
       "2  Jan 15 02:32:37 mail freshclam[29266]: Sat Jan...   \n",
       "3  Jan 15 02:32:42 mail freshclam[29266]: Sat Jan...   \n",
       "4  Jan 15 02:32:47 mail freshclam[29266]: Sat Jan...   \n",
       "\n",
       "                    @timestamp         location            id previous_output  \\\n",
       "0  2022-01-15T02:32:32.000000Z  /var/log/syslog  1.686147e+09             NaN   \n",
       "1  2022-01-15T02:32:32.000000Z  /var/log/syslog  1.686147e+09             NaN   \n",
       "2  2022-01-15T02:32:37.000000Z  /var/log/syslog  1.686147e+09             NaN   \n",
       "3  2022-01-15T02:32:42.000000Z  /var/log/syslog  1.686147e+09             NaN   \n",
       "4  2022-01-15T02:32:47.000000Z  /var/log/syslog  1.686147e+09             NaN   \n",
       "\n",
       "  filename predecoder_hostname predecoder_program_name predecoder_timestamp  \\\n",
       "0      fox                mail               freshclam      Jan 15 02:32:32   \n",
       "1      fox     taylorcruz-mail               freshclam      Jan 15 02:32:32   \n",
       "2      fox                mail               freshclam      Jan 15 02:32:37   \n",
       "3      fox                mail               freshclam      Jan 15 02:32:42   \n",
       "4      fox                mail               freshclam      Jan 15 02:32:47   \n",
       "\n",
       "          agent_ip  ... rule_mitre_tactic decoder_name decoder_parent  \\\n",
       "0    172.17.131.81  ...               NaN    freshclam            NaN   \n",
       "1  192.168.128.170  ...               NaN    freshclam            NaN   \n",
       "2    172.17.131.81  ...               NaN    freshclam            NaN   \n",
       "3    172.17.131.81  ...               NaN    freshclam            NaN   \n",
       "4    172.17.131.81  ...               NaN    freshclam            NaN   \n",
       "\n",
       "  decoder_ftscomment input_type GeoLocation_city_name  \\\n",
       "0                NaN        log                   NaN   \n",
       "1                NaN        log                   NaN   \n",
       "2                NaN        log                   NaN   \n",
       "3                NaN        log                   NaN   \n",
       "4                NaN        log                   NaN   \n",
       "\n",
       "  GeoLocation_country_name GeoLocation_region_name GeoLocation_location_lon  \\\n",
       "0                      NaN                     NaN                      NaN   \n",
       "1                      NaN                     NaN                      NaN   \n",
       "2                      NaN                     NaN                      NaN   \n",
       "3                      NaN                     NaN                      NaN   \n",
       "4                      NaN                     NaN                      NaN   \n",
       "\n",
       "  GeoLocation_location_lat  \n",
       "0                      NaN  \n",
       "1                      NaN  \n",
       "2                      NaN  \n",
       "3                      NaN  \n",
       "4                      NaN  \n",
       "\n",
       "[5 rows x 122 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ---------------- Flatten nested fields ----------------\n",
    "\n",
    "unflattened_cols = ['predecoder', 'agent', 'manager', 'data', 'rule', 'decoder', 'input', 'GeoLocation']\n",
    "flattened_dfs = []\n",
    "\n",
    "# Function to flatten the 'mitre' dictionary and prefix keys\n",
    "def flatten_mitre(m):\n",
    "    # Ensure m is a dict before iterating. Return empty dict for non-dict (like NaN)\n",
    "    if isinstance(m, dict):\n",
    "        return {f\"rule_mitre_{k}\": v for k, v in m.items()}\n",
    "    return {}\n",
    "\n",
    "for col in unflattened_cols:\n",
    "    if col in data.columns:\n",
    "        # Convert to list of dictionaries for json_normalize, handling NaNs\n",
    "        data_to_normalize = data[col].dropna().tolist()\n",
    "        \n",
    "        # Create a mapping from original index to the normalized data index\n",
    "        original_indices = data[col].dropna().index\n",
    "        \n",
    "        # Flatten the data\n",
    "        df_col = pd.json_normalize(data_to_normalize, sep='_')\n",
    "        \n",
    "        # Re-assign the original index to df_col so it aligns with data\n",
    "        df_col.index = original_indices\n",
    "        \n",
    "        print(f\"Columns in {col}:\", df_col.columns.tolist())\n",
    "        \n",
    "        if col == 'rule' and 'mitre' in df_col.columns:\n",
    "            # Apply flattening function to 'mitre' column and expand to new columns\n",
    "            mitre_expanded = df_col['mitre'].apply(flatten_mitre).apply(pd.Series)\n",
    "            \n",
    "            # Drop the original 'mitre' column\n",
    "            df_col = df_col.drop(columns=['mitre'])\n",
    "            \n",
    "            # Concatenate the expanded mitre columns. The indices will align.\n",
    "            df_col = pd.concat([df_col, mitre_expanded], axis=1)\n",
    "        \n",
    "        # Handle cases where the original series had NaNs and the resulting df_col is shorter\n",
    "        cols_to_rename = [c for c in df_col.columns if not c.startswith('rule_mitre_')]\n",
    "        df_col.rename(columns={c: f\"{col}_{c}\" for c in cols_to_rename}, inplace=True)\n",
    "        \n",
    "        flattened_dfs.append(df_col)\n",
    "\n",
    "# The key is that all DataFrames now share the same indices (or a subset thereof),\n",
    "# so pd.concat(axis=1) will align them correctly.\n",
    "df_flat = pd.concat([data.drop(columns=[c for c in unflattened_cols if c in data.columns])] + flattened_dfs, axis=1)\n",
    "\n",
    "df_flat.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2071aeb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save flattened data into a parquet file\n",
    "df_flat.to_parquet(path=os.path.join(extract_dir, parquet_filename))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
